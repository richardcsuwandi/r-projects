---
title: "Regression Analysis on Board Games Dataset"
output: rmarkdown::github_document
---
This project is one of my R projects series. In this project, we have a board games dataset from the weekly Tidy Tuesday event. The dataset contains 10,532 board games with at least 50 ratings between 1950 and 2016. A more detailed description of the dataset can be found [here](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-03-12)

The goal of this project is to perform a regression analysis on the board games dataset and build a model to predict the average ratings of the games.
```{r}
# Import the libraries
library(tidyverse)
library(broom)
theme_set(theme_light())
```

```{r}
# Import the data
board_games_raw <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv")

# The holdout set will be 20% of our raw data
holdout_set <- board_games_raw %>%
  filter(game_id %% 5 == 0)

# The training set will be 80% of our data
board_games <- board_games_raw %>%
  filter(game_id %% 5 != 0)
```

## Exploratory Data Analysis (EDA)
Let's first do some EDA on our data.

```{r}
# View the number of board game publishers
board_games_raw %>%
  count(publisher, sort = TRUE)
```

Looks like we have roughly 5500 distinct board game publishers. Also, some of them have more than 100 types of board games. Interesting!

```{r}
# Create a line plot for the number of board games by year published
board_games_raw %>%
  count(year_published) %>%
  ggplot(aes(year_published, n)) +
  geom_line()
```

We can see from the plot that our data seems to contain more recent data than the older ones. This might also indicate that there are way more board games recently than those of in the past.

```{r}
# Create a histogram for the average rating
board_games_raw %>%
  ggplot(aes(average_rating)) +
  geom_histogram()
```

From the histogram above, the average rating of the board games seems to be quite normally distributed around the 6.25 value. This is a good news if we might want to fit a model to predict these ratings.

```{r}
# View the number of board games by maximum playtime
board_games_raw %>%
  count(max_playtime, sort = TRUE)
```

From the table above, we can observe that most board games in our data have the maximum playtime of 30 minutes, followed by 60 minutes and 45 minutes respectively.

```{r}
# Create a tibble for the categorical variables
categorical_variables <- board_games %>%
  select(game_id, name, family, category, artist, designer, mechanic) %>%
  gather(type, value, -game_id, -name) %>%
  filter(!is.na(value)) %>%
  separate_rows(value, sep = ",") %>%
  arrange(game_id)
```

After making a tibble for the categorical variables, we can explore more about these variables.

```{r}
# Create a variable for the counts
categorical_counts <- categorical_variables %>%
  count(type, value, sort = TRUE)
categorical_counts
```

We can see that the most popular board game category is 'Card Game' with almost 2400 games while the most popular game mechanic being 'Dice Rolling' games. We can make a bar plot to visualize these findings better.

```{r}
# Create a bar plot for the top 10 board games categories
categorical_counts %>%
  filter(type == "category") %>%
  head(10) %>%
  mutate(category = fct_reorder(value, n)) %>%
  ggplot(aes(category, n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Game Category",
       y = "Number of Board Games",
       title = "The Most Popular Board Games Category")
```

The bar plot above clearly shows that 'Card Game' based board games are way more popular and common than the other categories, followed by 'Wargame' and 'Fantasy' respectively.

```{r}
library(drlib)

# Create a plot for the categorical variables
categorical_counts %>%
  group_by(type) %>%
  top_n(10, n) %>%
  ungroup() %>%
  mutate(value = reorder_within(value, n, type),
         type = fct_reorder(type, n, .desc = TRUE)) %>%
  ggplot(aes(value, n, fill = type)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ type, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Most Common Categorical Variables")
```

We can observe that the game mechanic and category are two of the most common categorical variables in our data. This could be useful as they might be our determining predictors for our model.

## Predicting the Average Ratings
Now, let's build a model to predict the average ratings.

```{r}
# Use the maximum players as the predictor
lm(average_rating ~ max_players, board_games) %>%
  summary()
```

From the summary above, it seems that the maximum barely has an effect on the average rating. However, let's try a log transformation.

```{r}
lm(average_rating ~ 
     log2(max_players + 1), board_games) %>%
  summary()
```

Now it's better! It seems that games with more players tend to get lower average ratings. Next, let's add maximum playtime to our predictors.

```{r}
lm(average_rating ~ 
     log2(max_players + 1) +
     log2(max_playtime +1), board_games) %>%
  summary()
```

The summary tells us that every time we double the number of maximum players, the average rating will decrease by approximately 0.15. On the other hand, if we double the amount of maximum playtime, the average rating will increase by roughly 0.13.

```{r}
# Create a plot for the average rating by decade
board_games %>%
  group_by(decade = 10 * (year_published %/% 10)) %>%
  summarize(average_rating = mean(average_rating)) %>%
  ggplot(aes(decade, average_rating)) +
  geom_line()
```

As we can see from the plot, there seems to be a general increasing trend here. This is to say that, newer games tend to get higher average ratings.

```{r}
# Add the year published to our predictors
lm(average_rating ~ 
     log2(max_players + 1) +
     log2(max_playtime +1) +
     year_published, board_games) %>%
  summary()
```

As we can see here, adding the year published to our predictors significantly reduce our residual standard error to 0.7464.

```{r}
# Create a tibble for the average rating based on categorical variables
board_games %>%
  inner_join(categorical_variables, by = c("game_id", "name")) %>%
  select(type, value, average_rating) %>%
  group_by(type, value) %>%
  summarize(games = n(),
            average_rating = mean(average_rating)) %>%
  arrange(desc(games))
```

Now, let's visualize it with a box plot to better understand it.

```{r}
# Create a box plot for the average rating by category
board_games %>%
  inner_join(categorical_variables, by = c("game_id", "name")) %>%
  filter(type == "category") %>%
  mutate(value = fct_lump(value, 15),
         value = fct_reorder(value, average_rating)) %>%
  ggplot(aes(value, average_rating)) +
  geom_boxplot() +
  coord_flip()
```

From the box plot above, we can see that World War II games get the highest average rating while children's games get the lowest.

```{r}
# Create a box plot for the average rating by mechanic
board_games %>%
  inner_join(categorical_variables, by = c("game_id", "name")) %>%
  filter(type == "mechanic") %>%
  mutate(value = fct_lump(value, 15),
         value = fct_reorder(value, average_rating)) %>%
  ggplot(aes(value, average_rating)) +
  geom_boxplot() +
  coord_flip()
```

The box plot above shows that simulation-based games get the highest rating. In contrast, the 'Roll / Spin and Move' games get a significantly lower average ratings than the others. Hence, we can conclude that categorical variables can be correlated with higher or lower rated games.

```{r}
# Create a tibble for the features
features <- categorical_variables %>%
  unite(feature, type, value) %>%
  add_count(feature) %>%
  filter(n >= 20)

# Import additional libraries
library(glmnet)
library(tidytext)
library(Matrix)
```

First, let's declare the predictors and the target for our regression model

```{r}
# Predictor
feature_matrix <- features %>%
  cast_sparse(game_id, feature)

# Target
ratings <- board_games$average_rating[match(rownames(feature_matrix), board_games$game_id)]
```

Now, let's fit our data to a Lasso Regression model using the glmnet library. Here, we will also be using the Cross-Validation (CV) method.

```{r}
# Fit the model and use the CV method
cv_lasso <- cv.glmnet(feature_matrix, ratings)
cv_lasso$glmnet.fit %>%
  tidy()
```

We can see how, in each step, different features are affecting our target with different coefficients. Also, notice that we have a parameter lambda to avoid overfitting the data.

```{r}
# View the summary of our model
cv_lasso
```

Let's visualize this to see how the MSE changes when the lambda values are changing.

```{r}
# Create a plot for the MSE and lambda
plot(cv_lasso)
```

From the plot above we can see that as the lambda value decreases, the MSE also tend to decrease. However, it seems that until it reaches a certain lambda value, the MSE will start to increase again. Let's use the “one-standard-error” rule to select the best model.

```{r}
# View the "one-standard-error" lambda value
cv_lasso$lambda.1se
```

Now, let's fit that lambda value to our model.

```{r}
cv_lasso$glmnet.fit %>%
  tidy() %>%
  filter(lambda == cv_lasso$lambda.1se) %>%
  arrange(desc(estimate))
```

From the results above, again, we can see how different features affect our target with different coefficients. For instance, the 'Urs Hostettler' designer has the highest positive weight while the 'Uncredited' designer has the highest negative weight.

Finally, we can add our non-categorical variables to our predictors
```{r}
# Create a tibble for the non-categorical variables
non_categorical_features <- board_games %>%
  transmute(game_id,
            name,
            year = year_published - 1950,
            log2_max_players = log2(max_players + 1),
            log2_max_playtime = log2(max_playtime + 1)) %>%
  gather(feature, value, -game_id, -name)

# Bind with the categorical variables
features <- categorical_variables %>%
  unite(feature, type, value, sep = ": ") %>%
  add_count(feature) %>%
  filter(n >= 20) %>%
  mutate(value = 1) %>%
  bind_rows(non_categorical_features)

# Add the non-categorical variables to our predictors
feature_matrix <- features %>%
  cast_sparse(game_id, feature, value)

ratings <- board_games$average_rating[match(rownames(feature_matrix), board_games$game_id)]
```

And now, we can fit our model with the combined categorical and non-categorical features.

```{r}
cv_lasso <- cv.glmnet(feature_matrix, ratings)
plot(cv_lasso)
```

To sum up, we can create a plot for the coefficients. 

```{r}
cv_lasso$glmnet.fit %>%
  tidy() %>%
  filter(lambda == cv_lasso$lambda.1se) %>%
  arrange(desc(estimate)) %>%
  filter(term != "(Intercept)") %>%
  top_n(25, abs(estimate)) %>%
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(term, estimate)) +
  geom_col() +
  coord_flip() +
  labs(title = "Largest Coefficients in Predicting Average Ratings of Board Games",
       subtitle = "Based on the Lasso Regression Model",
       x = "",
       y = "Coefficient")
```